identify(horsepower, mpg, name)
identify(horsepower, mpg, name)
summary(Auto)
detach(Auto)
## 1.2 Quadratic regression model
plot(mpg ~ horsepower, data=Auto)
lm.hp <- lm(mpg ~ horsepower, data=Auto)
abline(lm.hp)
lm.quad.hp <- lm(mpg ~ horsepower + I(horsepower ** 2), data=Auto)
abline(lm.quad.hp)
summary(lm.quad.hp)
fq <- function(x)
lm.quad.hp$coefficients[1] +
lm.quad.hp$coefficients[2]*x +
lm.quad.hp$coefficients[3]*x**2
curve(fq, 40, 230, add=TRUE)
# 1.3 Lab: Cross-validation
## 5.3.1 The Validation Set Approach
set.seed(1)
train=sample(392, 196)
train
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset = train)
attach(Auto)
library(boot)
library(ISLR)
set.seed(1)
train=sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset = train)
attach(Auto)
# ----- Cross-validation and bootstrapping -----
rm(list = ls())
# find.package('boot')
# install.packages('boot')
library(boot)
library(ISLR)
train=sample(392, 196)
# 1.3 Lab: Cross-validation
## 5.3.1 The Validation Set Approach
set.seed(1)
train=sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset = train)
attach(Auto)
# ----- Cross-validation and bootstrapping -----
rm(list = ls())
# 1.3 Lab: Cross-validation
## 5.3.1 The Validation Set Approach
set.seed(1)
train=sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset = train)
attach(Auto)
predict(lm.fit, Auto)
mpg - predict(lm.fit, Auto)
(mpg - predict(lm.fit, Auto)) ** 2
mean((mpg - predict(lm.fit, Auto)) ** 2)
attach(Auto)
mean((mpg - predict(lm.fit, Auto)) ** 2)
detach(Auto)
detach(Auto)
search()
detach(Auto)
detach(Auto)
search()
detach(Auto)
search()
set.seed(1)
train=sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset = train)
attach(Auto)
mean((mpg - predict(lm.fit, Auto)) ** 2)
mean((mpg - predict(lm.fit, Auto))[train] ** 2)
mean((mpg - predict(lm.fit, Auto))[-train] ** 2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data=Auto, subset = train) # quadratic
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data=Auto, subset = train) # cubic
mean((mpg - predict(lm.fit2, Auto))[-train] ** 2)
mean((mpg - predict(lm.fit3, Auto))[-train] ** 2)
set.seed(2)
train=sample(392, 196)
lm.fit  <- lm(mpg ~ horsepower,          data=Auto, subset = train) # linear
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data=Auto, subset = train) # quadratic
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data=Auto, subset = train) # cubic
mean((mpg - predict(lm.fit,  Auto))[-train] ** 2)
mean((mpg - predict(lm.fit2, Auto))[-train] ** 2)
mean((mpg - predict(lm.fit3, Auto))[-train] ** 2)
detach(Auto)
{
k = 5
lm.fit <- rep(NA, k)
MSE    <- rep(NA, k)
for(i in 1:k){
lm.fit[i] <- lm(mpg ~ poly(horsepower, i), data=Auto, subset = train)
attach(Auto)
MSE[i] <- mean((mpg - predict(lm.fit3, Auto))[-train] ** 2)
detach(Auto)
}
plot(1:k, MSE)
}
{
k = 5
# lm.fit <- rep(NA, k)
MSE    <- rep(NA, k)
for(i in 1:k){
lm.fit <- lm(mpg ~ poly(horsepower, i), data=Auto, subset = train)
attach(Auto)
MSE[i] <- mean((mpg - predict(lm.fit, Auto))[-train] ** 2)
detach(Auto)
}
plot(1:k, MSE)
}
plot(1:k, MSE, 'b')
# here I want to make sure that glm and lm sometimes give the same result
glm.fit <- glm(mpg ~ horsepower, data=Auto)
glm.fit
coef(glm.fit)
lm.fit  <- lm(mpg ~ horsepower, data=Auto)
coef(lm.fit)
# find.package('boot')
# install.packages('boot')
library(boot)
cv.err  <- cv.glm(Auto, glm.fit)
cv.err
cv.err$delta
{
k = 5
cv.error <- rep(0, k)
for(i in 1:k){
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
plot(1:k, cv.error)
}
## 5.3.3 k-Fold Cross-Validation
{
k = 10
cv.error <- rep(0, k)
for(i in 1:k){
glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error[i] <- cv.glm(Auto, glm.fit, K=k)$delta[1]
}
plot(1:k, cv.error, 'b')
}
# 2 Model Selection using cross-validation
glm.fit <- glm(mpg ~ poly(horsepower, 2), data = Auto)
cv.err  <- cv.glm(Auto, glm.fit)
cv.err$delta
# 2 Model Selection using cross-validation
{# defining a minimal model
glm.fit <- glm(mpg ~ poly(horsepower, 2), data = Auto)
cv.err  <- cv.glm(Auto, glm.fit)
cv.err$delta
}
# 2 Model Selection using cross-validation
{# defining a minimal model
glm.fit <- glm(mpg ~ poly(horsepower, 2), data = Auto)
cv.err  <- cv.glm(Auto, glm.fit)
cv.err$delta
foo = 5
}
foo
cv.error    <- rep(0, 6)
cv.error[1] <- cv.err$delta[1]
glm.fit     <- glm(mpg ~ poly(horsepower, 2) + year, data=Auto)
cv.error[2] <- cv.glm(Auto, glm.fit)$delta[1]
i = 0
cv.error    <- rep(0, 6)
cv.error[i<-(i+1)] <- cv.err$delta[1]
i
{ # Now add each of the other variables
i = 0
cv.error           <- rep(0, 6)
cv.error[i<-(i+1)] <- cv.err$delta[1]
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + weight, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + acceleration, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + displacement, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + cylinders, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
plot(cv.error, 'b')
}
plot(cv.error, 'b')
cv.error
plot(cv.error, type='b')
{ # Now add each of the other variables
i = 0; k = 6
cv.error <- rep(0, k)
label    <- rep(NA, k)
cv.error[i<-(i+1)] <- cv.err$delta[1]
label[i] <- "Nothing"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Year"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + weight, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Weight"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + acceleration, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Acceleration"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + displacement, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Displacement"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + cylinders, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Cylinders"
plot(label, cv.error, type='b')
}
label
plot(label, cv.error, type='b')
plot(label, cv.error, type='b')
plot(1:k, cv.error, type='b')
plot(as.factor(label), cv.error, type='b')
plot(as.factor(label), cv.error, type='l')
plot(as.factor(label), cv.error)
{ # Now add each of the other variables
i = 0; k = 5
cv.error <- rep(0, k)
label    <- rep(NA, k)
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year, data=Auto)
cv.error[i<-(i+1)] <- cv.err$delta[1]
label[i] <- "Nothing"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Weight"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + acceleration, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Acceleration"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + displacement, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Displacement"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + cylinders, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Cylinders"
plot(as.factor(label), cv.error)
}
{ # Now add each of the other variables
i = 0; k = 4
cv.error <- rep(0, k)
label    <- rep(NA, k)
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Nothing"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Acceleration"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + displacement, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Displacement"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + cylinders, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Cylinders"
plot(as.factor(label), cv.error)
}
{ # Now add each of the other variables
i = 0; k = 3
cv.error <- rep(0, k)
label    <- rep(NA, k)
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Nothing"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration + displacement, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Displacement"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration + cylinders, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Cylinders"
plot(as.factor(label), cv.error)
}
{ # Now add each of the other variables
i = 0; k = 2
cv.error <- rep(0, k)
label    <- rep(NA, k)
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration + displacement, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Nothing"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration + cylinders, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Cylinders"
plot(as.factor(label), cv.error)
}
{ # Now add each of the other variables
i = 0; k = 2
cv.error <- rep(0, k)
label    <- rep(NA, k)
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration + displacement, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Nothing"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration + displacement, cylinders, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Cylinders"
plot(as.factor(label), cv.error)
}
{ # Now add each of the other variables
i = 0; k = 2
cv.error <- rep(0, k)
label    <- rep(NA, k)
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration + displacement, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Nothing"
glm.fit            <- glm(mpg ~ poly(horsepower, 2) + year + weight + acceleration + displacement + cylinders, data=Auto)
cv.error[i<-(i+1)] <- cv.glm(Auto, glm.fit)$delta[1]
label[i] <- "Cylinders"
plot(as.factor(label), cv.error)
}
# Which new variable gives the lowest LOOCV MSE?
print("year + weight + acceleration + displacement + cylinders")
summary(glm.fit)
# 3 Programming Cross-Validation
rm(list=ls())
search()
rm(list = ls())
#install.packages('FNN')
library(ISLR)
library(corrplot)
install.packages(corrplot)
install.packages("corrplot")
library(corrplot)
library(pROC)
str(Smarket)
names(Smarket)
dim(Smarket)
summary(Smarket)
summary(Smarket)
cor(Smarket[, -9])
plot(Smarket$Volume)
glm.fits <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data = Smarket, family = binomial)
summary(glm.fits)
coef(glm.fits)
summary(glm.fits)$coef
glm.probs <- predict(glm.fits, type = "response")
glm.probs[1:10]
contrasts(Smarket$Direction)
glm.pred <- rep('Down', 1250)
glm.pred[glm.probs>.5] = 'Up'
table(glm.pred, Smarket$Direction)
glm.sensitivity = (507+145)/1250
glm.sensitivity = (507+145)/1250; glm.sensitivity
mean(glm.pred==Smarket$Direction)
train=(Smarket$Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Smarket$Direction[!train,]
Direction.2005=Smarket$Direction[!train,]
Direction.2005=Smarket$Direction[!train]
glm.fits <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
data = Smarket, family = binomial, subset = train)
glm.probs <- predict(glm.fits, Smarket.2005, type = "response")
table(glm.pred, Direction.2005)
glm.pred=rep('Down', 252)
glm.pred[glm.probs>.5] = 'Up'
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
glm.fits=glm(Direction~Lag1+Lag2,data=Smarket.2005, type="response")
glm.probs=predict(glm.fits, Smarket.2005, type="response")
glm.pred=rep("Down", 252)
glm.pred[glm.probs>.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
glm.fits=glm(Direction~Lag1+Lag2,data=Smarket, family = binomial,
subset = train)
glm.probs=predict(glm.fits, Smarket.2005, type="response")
glm.pred=rep("Down", 252)
glm.pred[glm.probs>.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
predict(glm.fits, newdata = data.frame(Lag1=c(1.2, 1.5),
Lag2=c(1.1, -.8)),
type = "response")
corrplot(cor(Smarket[, -9]))
# Exercise 2: Classifier for Diabetes ------------------------------------------
load("Diabetes.rda")
str(Diabetes)
dim(Diabetes)
row(Diabetes)
dim(Diabetes)[1]
str(Diabetes)
prop.table(Diabetes)
table(Diabetes)
table(Diabetes, 2)
table(Diabetes, 1)
View(Diabetes)
View(Diabetes)
table(Diabetes$YN)
prop.table(Diabetes$YN)
prop.table(table(Diabetes$YN))
apply(Diabetes, 2, mean)
?apply(Diabetes, 2, mean)
apply(Diabetes, 2, sum)
apply(Diabetes, 1, sum)
apply(Diabetes, 2, mean)
str(Diabetes)
apply(Diabetes[-1], 2, mean)
apply(Diabetes[-1], 2, sd)
hist(Diabetes)
hist(Diabetes$BMI)
library(ggplot2)
str(Diabetes)
ggplot(Diabetes,aes(x=BMI,group=YN,fill=type))+
geom_histogram(position="dodge",binwidth=0.25)+theme_bw()
ggplot(Diabetes,aes(x=BMI,group=YN,fill=YN))+
geom_histogram(position="dodge",binwidth=0.25)+theme_bw()
ggplot(Diabetes,aes(x=BMI,group=YN,fill=YN))+
geom_histogram(position="dodge",binwidth=1)+theme_bw()
ggplot(Diabetes,aes(x=BMI,group=YN,fill=Age))+
geom_histogram(position="dodge",binwidth=1)+theme_bw()
ggplot(Diabetes,aes(x=BMI,group=YN,fill=YN))+
geom_histogram(position="dodge",binwidth=1)+theme_bw()
ggplot(Diabetes,aes(x=Age,group=YN,fill=YN))+
geom_histogram(position="dodge",binwidth=1)+theme_bw()
ggplot(Diabetes,aes(x=BMI,group=YN,fill=YN))+
geom_histogram(position="dodge",binwidth=1)+theme_bw()
ggplot(Diabetes, aex(x=Age,y=BMI)) + geom_line()
ggplot(Diabetes, aes(x=Age,y=BMI)) + geom_line()
ggplot(Diabetes, aes(y=BMI,group=YN)) +
geom_boxplot()
ggplot(Diabetes, aes(y=BMI,x=YN)) +
geom_boxplot()
ggplot(Diabetes, aes(y=Age,x=YN)) + geom_boxplot()
ggplot(Diabetes, aes(y=BMI,x=YN)) + geom_boxplot()
unif(2)
#install.packages("pROC")
require(pROC)
# Split the data into a training and test data set, with 2000 observations in
# the test data set.
set.seed(50)
n<-dim(Diabetes)[1]
n
testidx<-sample(n,2000)
head(testidx)
test   <-Diabetes[testidx,]
train  <-Diabetes[-testidx,]
table(as.numeric(train$YN))
?sample
?sample()
table(as.numeric(Diabetes$YN))
592/7037
749/8880
# Fit the logistic regression model and look at the model summary.
glm.obj<-glm(YN~BMI,data=train,family="binomial")
summary(glm.obj)
glm.pred<-predict(glm.obj,newdata=data.frame(BMI=BMI.grid),type="response")
BMI.grid<-10:82
glm.pred<-predict(glm.obj,newdata=data.frame(BMI=BMI.grid),type="response")
plot(BMI.grid,glm.pred,type="l")
# Define ``High Risk of Diabetes'' using a cut off of alpha=0.5. Obtain the
# classification matrix
alpha<-0.5
fit1<-fitted(glm.obj)
HiRisk<-fit1>alpha
table(HiRisk)
table(train$YN,HiRisk)
prop.table(table(train$YN,HiRisk),1)
#Which is the sensitivity and which is the specificity for alpha=0.5
prop.table(table(train$YN,HiRisk),2)
sensitivity.di = prop.table(table(train$YN,HiRisk),2)[4]; sensitivity.di
specificity.di = prop.table(table(train$YN,HiRisk),2)[1]; specificity.di
##plot the ROC curve and obtain the AUC
require(pROC)
roc.obj1 <- roc(train$YN,fit1)
ggroc(roc.obj1)
auc(roc.obj1)
# roc produces a vector of thresholds (alpha), specificiteis and sensitivites.
# find the index of the threshold nearest to alpha=0.5
idx<-which.min(abs(roc.obj1$thresholds-alpha))
idx
roc.obj1$thresholds[idx]
roc.obj1$sensitivities[idx]
roc.obj1$specificities[idx]
str(roc.obj1)
roc.obj1$specificities[idx]
roc.obj1$thresholds[idx]
roc.obj1$sensitivities[idx]
roc.obj1$specificities[idx]
ptest<- predict(glm.obj, newdata=test, type="response")
test.roc.obj1 <- roc(test$YN,ptest)
ggroc(list(train=roc.obj1,test=test.roc.obj1))
auc(test.roc.obj1)
###Repeat for model with just Age
glm.obj2<-glm(YN~Age,data=train,family=binomial)
ptest2<- predict(glm.obj2, newdata=test, type="response")
roc.obj2 <- roc(train$YN, fitted(glm.obj2))
ggroc(list("BMI"=roc.obj1,"Age"=roc.obj2))
auc(roc.obj2)
###Repeat for model with BMI and Age
glm.obj3<-glm(YN~Age+BMI,data=train,family=binomial)
roc.obj2 <- roc(train$YN, ptest2)
roc.obj2 <- roc(train$YN, fitted(glm.obj2))
ggroc(list("BMI"=roc.obj1,"Age"=roc.obj2))
auc(roc.obj2)
#Add further commands here
roc.obj3 <- roc(train$YN, fitted(glm.obj3))
ggroc(list("BMI"=roc.obj1,"Age"=roc.obj2, "BMI+Age"=roc.obj3))
auc(roc.obj3)
# Define ``High Risk of Diabetes'' using a cut off of alpha=0.5. Obtain the
# classification matrix
alpha<-0.5
fit3<-fitted(glm.obj3)
HiRisk<-fit3>alpha
table(HiRisk)
table(train$YN,HiRisk)
prop.table(table(train$YN,HiRisk),1)
#Which is the sensitivity and which is the specificity for alpha=0.5
prop.table(table(train$YN,HiRisk),2)
sensitivity.di = prop.table(table(train$YN,HiRisk),2)[4]; sensitivity.di
specificity.di = prop.table(table(train$YN,HiRisk),2)[1]; specificity.di
exp(4)/(1+exp(4))
exp(1)/(1+exp(1))
exp(-100)/(1+exp(-100))
98/124
90/117
